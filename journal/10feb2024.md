# Top word embedding
**Concept:** Tokenize all text into small one-hot embeddings for the most common *n* (eg. 250) words in the English dictionary.

**Motivation:**
Interpretability of toy models.

**Test-cases:**
Can simple decoder lm predict simple observed patterns in text data.

[code](crates/top_word_embedding)
[data](https://github.com/Samyak2/toipe/blob/main/src/word_lists/top250)
