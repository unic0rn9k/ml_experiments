# Morphological tokenization
I have for a while had the feeling that my top tokenization scheme wasn't fit for my toy model use case, as the text it produces, even knowing the original text, seams pretty obscure.
This new specification should be a better option.
It is based on the idea of representing words as two tokens between 0 and 200, can represent 40,000 words, which is enough for a very complete dictionary.
I however want the tokens to also be easily interpretable, so (1,1) cant mean a completely different thing than (1,2), as it e.g. would be very hard to reason about a model's mis-inferences.

So my plan is to cluster all the words in my 37k dictionary's BERT embeddings, then have a rather large token size for the first token in a word-token pair (e.g. 800) and then a smaller second token size of 50.

**A contrived example that explains my reasoning behind this decision:**
A model outputs a token-pair (1,2), which corresponds to the word "red", but the expected output was "blue".
In this case we can reason about the decision by looking at all the words that can be created with an initial token of 1.
Here we might see that all the tokens are colors, and so perhaps this will reassure us that the model had the right idea, but got it wrong.

## How to finish this
- currently I am using the clustering crate to do kmeans, but it is much to slow for my use-case.
- I am planning to try to use kmediods instead, which looks fast and smart based on the readme (very good reasoning, I know)
- Here I will have to convert my word embeddings to a dissimilarity matrix ([r,c] = dist between embedding r and c)
- After I plan to export all the resulting data to a SCV file or smth, and do some data analysis in a python notebook with plotly
- I hope this won't take too long. I feel my current pace towards my toy models is very slow :,(
