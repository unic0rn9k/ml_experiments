# Meta
- created : 24. Feb 18:32 - 2024
- No Code
# MechInterp.
I have been reading about mechanistic interpretability, and have gotten some new intuitions on how transformer models work (and to some extent models in general).
I have found most of my material through [Neel Nanda's Mech-Interp glossary](https://www.neelnanda.io/mechanistic-interpretability/glossary).

## Features.
This section in mostly written in relation to the blog-post [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html#motivation).

- Models need to represent information internally.
- The smallest divisible piece of information in a model is referred to as a feature.
- How features are represented in a model is very debated, and most likely depends on the model and architecture.
- There is a consensus that models mostly learn to represent features as directions (interpretable basies).
- Other ways models could represent features (theoretically):
- Position in space
- Encoded into "binary" esque schemes (onhot encoding, would be an example of a very sparse way to do this)

## Residual streams.
Residual streams are not just some semi arbitrary way of making the models (not forget) information from previous layers.
It changes the paradigm of what the embeddings the decoder blocks are producing, to one of deltas from a central residual stream / original token.
This has implications that are significantly different from my original intuitions, specifically on how decoders learn.

I imagined that the optimization algorithm is some black box, that allows the model to learn how to represent new abstract representations on each layer, and having the transofmations in the representations be equivalent to some meaningful computation.

This means that if the optimizer would "want" to correct some erroneous behavior in the model, it would imply that all the following layers are operating under false pretenses.

My new intuition is that all layers are localy interpretable, because they work my mutating a global state, instead of operating in iteratively further abstracted spaces.

My new understanding implies both that mechanistic interpretability is possible, but also makes more sense in the case of an optimization algorithm, that works by moving in a continuously sloped cost space
(oppesed to having a disconnect between discrete information represented as matrices, and an optimzer that is fighting the architecture, and is just sort of shoe horned into fixing the problem).


This different understanding also implies that my `experiment_1` [5feb2024](5feb2024.md), is fundamentally flawed, as it discrete-tizes the internal features in a way that I now imagine will make it harder for the optimizer to do its thing. I'm not sure if that makes the project less interesting to explore tho, as it might help me shed light on what exactly makes a model able to converge.


# Conclusion
These new insights, have motivated me to firstly try to build out my core decoder implementations,
and make some mech-interp esque experiments, testing my different internal models of how decoders learn.
After having done this, I want to create a toy mixture-of-experts model, and try to verify that it is triggering specific experts in response to specific interpretable features.
