It is my impression that the contrived probing model from [my last commit](https://github.com/unic0rn9k/ml_experiments/blob/5e144559c946fff5793351bb42e1c976fe499e7e/journal/14mar2024.md)
is failing to predict the token probabilities based on the embeddings, because of too few training iterations.
I base this on the fact that the model in this commit, is able to converge on the desired transformation, which again its still just a linear projection optimized with adam and mse, but this time trying to predict a much smaller set of labels.

The reason why I was only training it for so few iterations was because it was too slow.
This makes me question again: Is candle just kinda slow? This would be a bummer I think.
I'd like to do a test of the performance of my model from the last commit against a hand-crafted gradient descent algorithm,
that just predicts noise, but with all parameters/variables in the same shapes of course.

(keep in mind I am still running on CPU, for now...)
