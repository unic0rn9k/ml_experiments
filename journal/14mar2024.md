# Experiments I would like to do
## Other simplified tokenizers
By simplified tokenizer I mean something in the line of my [top word tokenizer](/journal/10feb2024.md).
Some alternatives that would also be useful for my interpretability ventures would be:

- The same thing, but taking a larger context into account when tokenizing words, for more accurate simplifications. This should be easy to do with BERT.

- **Domain Specific Sementics:** Imagine a language where each token carries more meaning than what a single word or sub-word typically would, 
but which is still interpretable for humans, and the machine.

## Sparsification
- **Sparse Output Representation:** Using AST-like output post-proccessing to only include specific subset of possible output tokens in sampling, and when calculating loss.
This way the model can also store a bunch of information at invalid token position, without being penalized (given that the model itself can also model the AST, which again implies that this could be improved by also embedding the AST into the model inputs).

- **Sparse Heads:** I also imagine that it would be possible to use top-k sampling in attention scores, instead of just softmax, and then index into the value matrix, instead of using matrix multiplication.
Also think it would be cool to replicate the mixture of experts paper.


# How I'm aproaching the first steps
1. Using top word tokenizations to train some simple toy decoder models on some text and see if I can get them to actually produce semantically meaningful results.
2. Then trying to mess around with different parameters and things, to see how this will affect performance.
3. After having compiled a couple of different models (maybe 3),
I would like to try to run through some examples and figure out specific cases of things that are hard for some of the models,
but not the others (and maybe also hard for all of them).
4. Then create some sort of LM fuzzer, using BERT masked sentence prediction (perhaps in combination with other BERT based approaches),
to figure out where the line is when the models break.
5. Try to do mechanistic interpretability when having some data on where to look?
I'm not really to sure I know how to do this yet, but I suspect I'll have some ideas when I get here.
Otherwise, I guess I'll know what to research then :)

# Some of my struggles right now (very technically uninteresting)
I was trying to create [an experiment](/src/experiment_2.rs) on the 3rd of march.
I wanted to get a probability distribution over a desired set of tokens, from a masked token prediction model.
This is not possible through the [rust-bert](https://lib.rs/rust-bert) API, I suspect because they do some sort of BEAM sampling, to make it output multiple tokens, if that seems more likely for a given mask.
Since I wanted to potentially use this model with one of my own tokenization schemes.
I thought a single linear layer on top of some BERT based model would be fine, but I couldn't get it to produce anything useful,
even for a simple test where I was just trying to get it to overfit.

Additionally, it was very time-consuming getting it to train, because I initially couldn't get [candle](https://github.com/huggingface/candle) to use my laptops GPU.
I do also have a headless stationary computer, but it seamed like unnecessary, plus I haven't set it up yet :/
Seams weird to me that it should be that slow to optimize a linear layer, even if it is pretty big (384x30522+384 parameters)
